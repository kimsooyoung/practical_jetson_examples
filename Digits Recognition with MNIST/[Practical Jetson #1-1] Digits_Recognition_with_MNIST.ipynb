{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsooyoung/practical_jetson_examples/blob/main/Digits%20Recognition%20with%20MNIST/%5BPractical%20Jetson%20%231-1%5D%20Digits_Recognition_with_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNe040agAkEB"
      },
      "source": [
        "# **Practical Exercise with MNIST Example #1-1 - \"Train Part\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrJrZ-yTBT9i"
      },
      "source": [
        "**[!important] Check your Runtime type before  running this example (Runtime > Change runtime type)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNYWCEfrAkEE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import easydict\n",
        "import numpy as np\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJYBuMzpZjdg"
      },
      "source": [
        "# **GPU usability check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOpdUKT5g9RM",
        "outputId": "66bed5da-e38a-450e-97ea-0841b7b8e1c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSbzxlJnZT1X"
      },
      "source": [
        "# **Model Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_tkizn-g2iI"
      },
      "source": [
        "* In this code, simple `Block` is defined for reusable code. This will help a lot for saving time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGJHEvVKgzr2"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                input_size,\n",
        "                output_size,\n",
        "                use_batch_norm=True,\n",
        "                dropout_p=.4):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.dropout_p = dropout_p\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        def get_regularizer(use_batch_norm, size):\n",
        "            # BatchNorm1d: Convert one MNIST dataset into 784 dim vector (784 means node for fully connected layer)\n",
        "            return nn.BatchNorm1d(size) if use_batch_norm else nn.Dropout(dropout_p)\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Linear(input_size, output_size),\n",
        "            nn.LeakyReLU(),\n",
        "            get_regularizer(use_batch_norm, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, input_size)\n",
        "        y = self.block(x)\n",
        "        # |y| = (batch_size, output_size)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV_uf9UDg5vY"
      },
      "source": [
        "* Define actual model named as `ImageClassifier`\n",
        "* This structure is referenced from [this book](https://www.yes24.com/Product/Goods/112198327) and [this link](https://kh-kim.github.io/nlp_with_deep_learning_blog/docs/1-15-practical_exercise/08-predict_exercise/)\n",
        "\n",
        "> Copyright Â© 2017-2020 Patrick Marsceill. Distributed by an MIT license"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qqgGQWAg5_c"
      },
      "outputs": [],
      "source": [
        "class ImageClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                input_size,\n",
        "                output_size,\n",
        "                hidden_sizes=[500, 400, 300, 200, 100],\n",
        "                use_batch_norm=True,\n",
        "                dropout_p=.4):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        assert len(hidden_sizes) > 0, \"You need to specify hidden layers\"\n",
        "\n",
        "        last_hidden_size = input_size\n",
        "        blocks = []\n",
        "        for hidden_size in hidden_sizes:\n",
        "            blocks += [Block(\n",
        "                last_hidden_size,\n",
        "                hidden_size,\n",
        "                use_batch_norm,\n",
        "                dropout_p\n",
        "            )]\n",
        "            last_hidden_size = hidden_size\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            *blocks,\n",
        "            nn.Linear(last_hidden_size, output_size),\n",
        "            nn.LogSoftmax(dim=-1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # |x| = (batch_size, input_size)\n",
        "        y = self.layers(x)\n",
        "        # |y| = (batch_size, output_size)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvGMYMSLZFyj"
      },
      "source": [
        "# **Train helper Implemenation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5h63jROZcju"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "\n",
        "    def __init__(self, model, optimizer, crit):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.crit = crit\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def _batchify(self, x, y, batch_size, random_split=True):\n",
        "        if random_split:\n",
        "            indices = torch.randperm(x.size(0), device=x.device)\n",
        "            x = torch.index_select(x, dim=0, index=indices)\n",
        "            y = torch.index_select(y, dim=0, index=indices)\n",
        "\n",
        "        x = x.split(batch_size, dim=0)\n",
        "        y = y.split(batch_size, dim=0)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def _train(self, x, y, config):\n",
        "        # Turn train mode on. (default: model.train)\n",
        "        self.model.train()\n",
        "\n",
        "        x, y = self._batchify(x, y, config.batch_size)\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
        "            y_hat_i = self.model(x_i)\n",
        "            loss_i = self.crit(y_hat_i, y_i.squeeze())\n",
        "\n",
        "            # Initialize the gradients of the model.\n",
        "            self.optimizer.zero_grad()\n",
        "            loss_i.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if config.verbose >= 2:  # print loss for two more config verbose\n",
        "                print(\"Train Iteration(%d/%d): loss=%.4e\" % (i + 1, len(x), float(loss_i)))\n",
        "\n",
        "            # Don't forget to detach to prevent memory leak.\n",
        "            total_loss += float(loss_i)\n",
        "\n",
        "        return total_loss / len(x)\n",
        "\n",
        "    def _validate(self, x, y, config):\n",
        "        # Turn evaluation mode on.\n",
        "        self.model.eval()\n",
        "\n",
        "        # Turn on the no_grad mode to make more efficintly.\n",
        "        with torch.no_grad():\n",
        "            x, y = self._batchify(x, y, config.batch_size, random_split=False)\n",
        "            total_loss = 0\n",
        "\n",
        "            for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
        "                y_hat_i = self.model(x_i)\n",
        "                loss_i = self.crit(y_hat_i, y_i.squeeze())\n",
        "\n",
        "                if config.verbose >= 2:\n",
        "                    print(\"Valid Iteration(%d/%d): loss=%.4e\" % (i + 1, len(x), float(loss_i)))\n",
        "\n",
        "                total_loss += float(loss_i)\n",
        "\n",
        "            return total_loss / len(x)\n",
        "\n",
        "    def train(self, train_data, valid_data, config):\n",
        "\n",
        "        # save best loss and model at then\n",
        "        lowest_loss = np.inf\n",
        "        best_model = None\n",
        "\n",
        "        for epoch_index in range(config.n_epochs):\n",
        "            train_loss = self._train(train_data[0], train_data[1], config)\n",
        "            valid_loss = self._validate(valid_data[0], valid_data[1], config)\n",
        "\n",
        "            # You must use deep copy to take a snapshot of current best weights.\n",
        "            if valid_loss <= lowest_loss:\n",
        "                lowest_loss = valid_loss\n",
        "                best_model = deepcopy(self.model.state_dict())\n",
        "\n",
        "            print(\"Epoch(%d/%d): train_loss=%.4e  valid_loss=%.4e  lowest_loss=%.4e\" % (\n",
        "                epoch_index + 1,\n",
        "                config.n_epochs,\n",
        "                train_loss,\n",
        "                valid_loss,\n",
        "                lowest_loss,\n",
        "            ))\n",
        "\n",
        "        # Restore to best model.\n",
        "        self.model.load_state_dict(best_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-6ldz-yg7Oh"
      },
      "source": [
        "# **DataLoader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEgCtba8g84Y"
      },
      "outputs": [],
      "source": [
        "def load_mnist(is_train=True, flatten=True):\n",
        "\n",
        "    dataset = datasets.MNIST('../data',      # ../data: path for MNIST Dataset\n",
        "        train=is_train,               # is_train  load train dataset for True, load test dataset for False\n",
        "        download=True,\n",
        "        transform=transforms.Compose([\n",
        "            transforms.ToTensor(),         # convert image info tensor using transforms.ToTensor()\n",
        "        ]),\n",
        "    )\n",
        "\n",
        "    x = dataset.data.float() / 255.   # / 255 : From 255 level pixel value into 0-1 scale\n",
        "    y = dataset.targets               # ground truth tensor\n",
        "\n",
        "    if flatten:\n",
        "        x = x.view(x.size(0), -1)    # 28*28 = 784 (2D image into 1D vector)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def split_data(x, y, train_ratio=.8):\n",
        "\n",
        "    train_cnt = int(x.size(0) * train_ratio)\n",
        "    valid_cnt = x.size(0) - train_cnt\n",
        "\n",
        "    # Shuffle dataset to split into train/valid set.\n",
        "    indices = torch.randperm(x.size(0))\n",
        "    x = torch.index_select(\n",
        "        x,\n",
        "        dim=0,\n",
        "        index=indices\n",
        "    ).split([train_cnt, valid_cnt], dim=0)\n",
        "\n",
        "    y = torch.index_select(\n",
        "        y,\n",
        "        dim=0,\n",
        "        index=indices\n",
        "    ).split([train_cnt, valid_cnt], dim=0)\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "def get_hidden_sizes(input_size, output_size, n_layers):\n",
        "    step_size = int((input_size - output_size) / n_layers)\n",
        "\n",
        "    hidden_sizes = []\n",
        "    current_size = input_size\n",
        "    for i in range(n_layers - 1):\n",
        "        hidden_sizes += [current_size - step_size]\n",
        "        current_size = hidden_sizes[-1]\n",
        "\n",
        "    return hidden_sizes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpN-EDpWZtwW"
      },
      "source": [
        "# **Main Loop and Final Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLa_xvF8g0TS"
      },
      "outputs": [],
      "source": [
        "# handle trian configuration throught EasyDict\n",
        "def define_argparser():\n",
        "\n",
        "    args = easydict.EasyDict({\n",
        "        \"model_fn\" : './model_test.pth',\n",
        "        \"gpu_id\": 0,\n",
        "\n",
        "        \"train_ratio\": 0.8,\n",
        "\n",
        "        \"batch_size\": 256,\n",
        "        \"n_epochs\": 10,\n",
        "\n",
        "        \"n_layers\": 5,\n",
        "        \"use_dropout\": 'store_true',\n",
        "        \"dropout_p\": 0.3,\n",
        "\n",
        "        \"verbose\":1\n",
        "        })\n",
        "\n",
        "    config = args\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def main(config):\n",
        "    # Set device based on user defined configuration.\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    x, y = load_mnist(is_train=True, flatten=True)\n",
        "\n",
        "    x, y = split_data(x, y, train_ratio=config.train_ratio)\n",
        "    print(\"Train:\", x[0].shape, y[0].shape)\n",
        "    print(\"Valid:\", x[1].shape, y[1].shape)\n",
        "\n",
        "    input_size = int(x[0].shape[-1])\n",
        "    output_size = int(max(y[0])) + 1\n",
        "\n",
        "    model = ImageClassifier(\n",
        "        input_size=input_size,\n",
        "        output_size=output_size,\n",
        "        hidden_sizes=get_hidden_sizes(input_size,\n",
        "                                    output_size,\n",
        "                                    config.n_layers),\n",
        "        use_batch_norm=not config.use_dropout,\n",
        "        dropout_p=config.dropout_p,\n",
        "    ).to(device)\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    crit = nn.NLLLoss()\n",
        "\n",
        "    if config.verbose > 1:\n",
        "        print(model)\n",
        "        print(optimizer)\n",
        "        print(crit)\n",
        "\n",
        "    trainer = Trainer(model, optimizer, crit)\n",
        "\n",
        "    train_x, train_y = x[0].to(device), y[0].to(device)\n",
        "    valid_x, valid_y = x[1].to(device), y[1].to(device)\n",
        "\n",
        "    trainer.train(\n",
        "        train_data=(train_x, train_y),\n",
        "        valid_data=(valid_x, valid_y),\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Save best model weights.\n",
        "    torch.save({\n",
        "        'model': trainer.model.state_dict(),\n",
        "        'opt': optimizer.state_dict(),\n",
        "        'config': config,\n",
        "    }, config.model_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww0jD4abEqfn"
      },
      "source": [
        "## **Start Training and Export `.pth` file from Files tab in the left side**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-zBnQRKZzoW",
        "outputId": "78d0e42f-4b0a-4f39-9d20-64023396a2ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: torch.Size([48000, 784]) torch.Size([48000])\n",
            "Valid: torch.Size([12000, 784]) torch.Size([12000])\n",
            "Epoch(1/10): train_loss=5.0496e-01  valid_loss=1.6475e-01  lowest_loss=1.6475e-01\n",
            "Epoch(2/10): train_loss=1.6713e-01  valid_loss=1.2158e-01  lowest_loss=1.2158e-01\n",
            "Epoch(3/10): train_loss=1.2270e-01  valid_loss=9.5086e-02  lowest_loss=9.5086e-02\n",
            "Epoch(4/10): train_loss=9.9114e-02  valid_loss=8.9470e-02  lowest_loss=8.9470e-02\n",
            "Epoch(5/10): train_loss=8.0746e-02  valid_loss=7.7420e-02  lowest_loss=7.7420e-02\n",
            "Epoch(6/10): train_loss=7.1243e-02  valid_loss=7.7732e-02  lowest_loss=7.7420e-02\n",
            "Epoch(7/10): train_loss=5.9378e-02  valid_loss=7.0156e-02  lowest_loss=7.0156e-02\n",
            "Epoch(8/10): train_loss=5.6176e-02  valid_loss=7.0591e-02  lowest_loss=7.0156e-02\n",
            "Epoch(9/10): train_loss=5.0510e-02  valid_loss=6.6637e-02  lowest_loss=6.6637e-02\n",
            "Epoch(10/10): train_loss=4.4377e-02  valid_loss=7.7233e-02  lowest_loss=6.6637e-02\n"
          ]
        }
      ],
      "source": [
        "config = define_argparser()\n",
        "main(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVrolAZSE6d5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
