{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsooyoung/practical_jetson_examples/blob/main/Digits%20Recognition%20with%20MNIST/%5BPractical%20Jetson%20%231-4%5D%20Digits_Recognition_with_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtruPxh4RkUK"
      },
      "source": [
        "# **Practical Exercise with MNIST Example #1-4 - \"Predict with CNN Model\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FtKdby1sM-t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Colab only\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GENE7BEMSuDP"
      },
      "source": [
        "## **GPU usability check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdUy1uIBSwzh",
        "outputId": "1c17434b-6d19-4fc8-b470-fb17da90e471"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txD6QoI0Snyp"
      },
      "source": [
        "## **Same Model as Before**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr564_BasM-z"
      },
      "outputs": [],
      "source": [
        "class CNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "\n",
        "        #### Layer1 ####\n",
        "        # Image Input shape = (Data Length, Width, Height, Channel)\n",
        "        # Input shape = (N, 28, 28, 1)\n",
        "        # Conv = (N, 28, 28, 32)\n",
        "        # Pool = (N, 14, 14, 32)\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        #### Layer2 ####\n",
        "        # Input shape = (N, 14, 14, 32)\n",
        "        # Conv = (N, 14, 14, 64)\n",
        "        # Pool = (N, 7, 7, 64)\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        #### Layer3 ####\n",
        "        # Fully connected layer\n",
        "        # Input shape = (N, 7, 7, 64)\n",
        "        # Output = (N, 10)\n",
        "        self.fc = torch.nn.Linear(7 * 7 * 64, 10, bias=True)\n",
        "\n",
        "        # Initialize weights with Xavier initialization to the fully connected layer only\n",
        "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
        "\n",
        "    # forward pass\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0), -1)   # flatten for fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFcx0NHrT5Yd"
      },
      "source": [
        "## **Load pretained parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkM6J15ksM-0"
      },
      "outputs": [],
      "source": [
        "def load_model(model_path):\n",
        "    model = CNN()\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Set the model to Inference Mode\n",
        "    model.eval()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95_u8eVCUO3C"
      },
      "source": [
        "**[!important] You must have to import a trained `.pth` file in this workspace!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnp-nMfssM-0",
        "outputId": "3f7b252e-ee90-4576-c522-4ee3d7154063"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model File Found and Loaded Keep Going :)\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "model_path = \"./CNN_trained_model.pth\"\n",
        "\n",
        "if os.path.isfile(model_path):\n",
        "  print(\"Model File Found and Loaded Keep Going :)\")\n",
        "  model = load_model(model_path)\n",
        "else:\n",
        "  print(\"[ERROR] No Model File Found :(\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqED1aCSsM-1"
      },
      "source": [
        "## **Handwritten Digit Recognition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEb0roMZsM-1"
      },
      "outputs": [],
      "source": [
        "def preprocess_image_opencv(image_path):\n",
        "    ''' The is a function that for one image preprocesses it.'''\n",
        "\n",
        "    img_gray = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Color to grayscale\n",
        "    img_gray = cv2.GaussianBlur(img_gray, (5, 5), 0)         # Gaussian Kernel, kernel size (5, 5)\n",
        "\n",
        "    # Adaptive threshold processing better with variable lighting:\n",
        "    # ref : https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html\n",
        "    # cv2.ADAPTIVE_THRESH_GAUSSIAN_C: Adaptive threshold processing method using Gaussian weighted averages\n",
        "    # cv2.THRESH_BINARY_INV: Binary conversion and invert black & white. (Original datasets have white letters, so need to be inverted)\n",
        "    final_img = cv2.adaptiveThreshold(img_gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, blockSize = 321, C = 28)\n",
        "\n",
        "    # visualize image\n",
        "    cv2_imshow(final_img)\n",
        "\n",
        "    # image resize for model input\n",
        "    image = cv2.resize(final_img, (28, 28))\n",
        "\n",
        "    # image to PyTorch Tensor\n",
        "    image_tensor = transforms.ToTensor()(image).unsqueeze(0)\n",
        "\n",
        "    return image_tensor\n",
        "\n",
        "\n",
        "def predict(model, image_tensor):\n",
        "    '''Uses the CNN model to predict the number of images'''\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        output = model(image_tensor)\n",
        "\n",
        "        _, predicted_class = torch.max(output, 1)\n",
        "\n",
        "        # .item() : vector to scalar\n",
        "        return predicted_class.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dPmRiwtUzuI"
      },
      "source": [
        "## **Change image path/name and run prediction!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H57FdDiuVgKV"
      },
      "source": [
        "**[!important] You must specify image file name and path in this workspace!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe3oEEjFVgod",
        "outputId": "dcd7ae9c-cb5e-4f95-f624-4b863d08466c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image File Found Keep Going :)\n"
          ]
        }
      ],
      "source": [
        "import os.path\n",
        "\n",
        "image_path = './demo_3.png'\n",
        "\n",
        "if os.path.isfile(image_path):\n",
        "  print(\"Image File Found Keep Going :)\")\n",
        "else:\n",
        "  print(\"[ERROR] No Image File :(\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "Da-00IK5sM-2",
        "outputId": "e31692b7-2ddc-4846-c8ba-8fb5573ba6dd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=282x352>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "./demo_3.png Predicted class: 3\n"
          ]
        }
      ],
      "source": [
        "''' Pre-processing and prediction of images '''\n",
        "\n",
        "image_tensor = preprocess_image_opencv(image_path)\n",
        "predicted_class = predict(model, image_tensor.to(device))\n",
        "print(f\"{image_path} Predicted class: {predicted_class}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjM6f6OVsM-2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
